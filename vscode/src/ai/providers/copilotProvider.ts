import * as vscode from "vscode";
import type { GenerateOptions, StreamGenerateOptions } from "../aiInterface";
import type { SvnFile } from "../../vcs/svnService";
import { setScmInputBoxValue } from "../../utils/setScmInputBoxValue";
import { BaseProvider } from "./baseProvider";
import { PROVIDER_NAMES } from "../utils/constants";
import { buildBasePrompt, buildBugReasonPrompt } from "../utils/buildPrompt";
import { extractCommitMessage } from "../utils/extractCommitMessage";
import { enforceConventionalCommit } from "../utils/enforceConventionalCommit";
import { handleApiError } from "../utils/handleApiError";
import { outputChannel } from "../../utils/outputChannel";

/**
 * æµå¼ç”Ÿæˆæäº¤ä¿¡æ¯å¹¶å®æ—¶æ›´æ–°åˆ°SCMè¾“å…¥æ¡†
 */

export class CopilotProvider extends BaseProvider {
  readonly name = PROVIDER_NAMES.COPILOT;

  async isAvailable(): Promise<boolean> {
    try {
      let model = await this.getAvailableModel();
      if (model) {
        const messages = [
          vscode.LanguageModelChatMessage.User("æµ‹è¯•å¯ç”¨æ€§,è¯·å¿½ç•¥æ­¤æ¶ˆæ¯ã€‚"),
        ];

        const response = await model.sendRequest(
          messages,
          {},
          new vscode.CancellationTokenSource().token,
        );

        let result = "";
        for await (const fragment of response.text) {
          result += fragment;
        }
        return result.trim().length > 0;
      }
      return false;
    } catch (error) {
      console.error(`${PROVIDER_NAMES.COPILOT}å¯ç”¨æ€§æ£€æŸ¥å¤±è´¥:`, error);
      return false;
    }
  }

  async generateCommitMessage(
    diff: string,
    changedFiles: SvnFile[],
    options?: StreamGenerateOptions,
  ): Promise<string> {
    try {
      let model = await this.getAvailableModel();

      const prompt = buildBasePrompt(diff, changedFiles, options);
      const messages = [vscode.LanguageModelChatMessage.User(prompt)];

      const response = await model.sendRequest(
        messages,
        {},
        new vscode.CancellationTokenSource().token,
      );

      let result = "";
      for await (const fragment of response.text) {
        result += fragment;
      }
      const raw = extractCommitMessage(result.trim());
      return enforceConventionalCommit(
        raw,
        changedFiles,
        diff,
        options?.zendaoInfo,
      );
    } catch (error) {
      handleApiError(error, PROVIDER_NAMES.COPILOT);
    }
  }

  async generateCommitMessageWithStream(
    diff: string,
    changedFiles: SvnFile[],
    options?: StreamGenerateOptions,
  ): Promise<string> {
    try {
      let model = await this.getAvailableModel();

      // æ„å»ºæç¤ºä¿¡æ¯
      const prompt = buildBasePrompt(diff, changedFiles, options);
      const messages = [vscode.LanguageModelChatMessage.User(prompt)];

      // å¼€å§‹æµå¼è¯·æ±‚
      const response = await model.sendRequest(
        messages,
        {},
        new vscode.CancellationTokenSource().token,
      );
      const debug = vscode.workspace
        .getConfiguration("aiMessage")
        .get<boolean>("debug.enableStreamingLog", false);
      if (debug) {
        console.log("[AI-Message][Stream] å¯åŠ¨æµå¼ï¼Œä¼šè¯æ¨¡å‹:", model.id);
      }

      let result = "";
      let lastUpdateTime = Date.now();
      const updateInterval = 200; // æ¯200msæ›´æ–°ä¸€æ¬¡ç•Œé¢
      let fragmentCount = 0;
      let firstChunkTime: number | null = null;
      const startTime = Date.now();

      for await (const fragment of response.text) {
        fragmentCount++;
        if (firstChunkTime === null) {
          firstChunkTime = Date.now();
        }
        result += fragment;
        if (debug) {
          console.log(
            `[AI-Message][Stream] ç‰‡æ®µ#${fragmentCount} é•¿åº¦=${fragment.length} ç´¯è®¡=${result.length}`,
          );
        }

        // å®šæœŸæ›´æ–°è¾“å…¥æ¡†ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„UIæ›´æ–°
        const now = Date.now();
        if (now - lastUpdateTime > updateInterval) {
          const displayText =
            result.length > 10
              ? `ğŸ¤– AIæ­£åœ¨ç”Ÿæˆ...\n\n${result}${
                  result.endsWith("\n") ? "" : "..."
                }`
              : "ğŸ¤– AIæ­£åœ¨æ€è€ƒ...";

          const ok = await setScmInputBoxValue(displayText);
          if (debug && !ok && !options?.fallbackToOutput) {
            console.log(
              "[AI-Message][Stream] SCMå†™å…¥å¤±è´¥ä½†æœªå¯ç”¨fallbackToOutput",
            );
          }
          if (!ok && options?.fallbackToOutput) {
            outputChannel.show(true);
            outputChannel.replace
              ? outputChannel.replace(displayText)
              : (function () {
                  // æ²¡æœ‰replaceæ–¹æ³•æ—¶ç®€å•æ¸…å±å†å†™
                  outputChannel.clear();
                  outputChannel.append(displayText);
                })();
            if (debug) {
              console.log(
                "[AI-Message][Stream] å·²å†™å…¥OutputChannel (fallback)",
              );
            }
          }
          lastUpdateTime = now;

          // æ›´æ–°è¿›åº¦
          const progressIncrement = Math.min(85 + result.length / 10, 95);
          options?.progress.report({
            increment: progressIncrement,
            message: "å®æ—¶ç”Ÿæˆä¸­...",
          });
        }
      }

      // æœ€ç»ˆå¤„ç†å’Œè®¾ç½®å®Œæ•´ç»“æœ
      if (result.trim()) {
        if (debug) {
          const totalMs = Date.now() - startTime;
          const ttfb = firstChunkTime ? firstChunkTime - startTime : -1;
          console.log(
            `[AI-Message][Stream] å®Œæˆï¼Œæ€»ç‰‡æ®µ=${fragmentCount}, æ€»é•¿åº¦=${result.length}, é¦–å­—èŠ‚(ms)=${ttfb}, æ€»è€—æ—¶(ms)=${totalMs}`,
          );
        }
        // æå–æäº¤ä¿¡æ¯ï¼ˆå»æ‰å¯èƒ½çš„å‰ç¼€å’Œæ ¼å¼ï¼‰
        const raw = extractCommitMessage(result.trim());
        const formatted = enforceConventionalCommit(
          raw,
          changedFiles,
          diff,
          options?.zendaoInfo,
        );
        const finalOk = await setScmInputBoxValue(formatted);
        if (!finalOk && options?.fallbackToOutput) {
          outputChannel.show(true);
          outputChannel.appendLine("\n=== æœ€ç»ˆæäº¤ä¿¡æ¯ ===");
          outputChannel.appendLine(formatted);
          if (debug) {
            console.log("[AI-Message][Stream] æœ€ç»ˆç»“æœå†™å…¥OutputChannel");
          }
        }
        options?.progress.report({ increment: 100, message: "å®Œæˆï¼" });
        return formatted;
      } else {
        throw new Error("ç”Ÿæˆçš„å†…å®¹ä¸ºç©º");
      }
    } catch (error) {
      console.error(`${PROVIDER_NAMES.COPILOT}æµå¼ç”Ÿæˆå¤±è´¥:`, error);
      throw error;
    }
  }

  async generateReason(
    diff: string,
    changedFiles: SvnFile[],
    options?: GenerateOptions,
  ): Promise<string> {
    let model = await this.getAvailableModel();

    const prompt = buildBugReasonPrompt(diff, changedFiles, options);
    const messages = [vscode.LanguageModelChatMessage.User(prompt)];

    const response = await model.sendRequest(
      messages,
      {},
      new vscode.CancellationTokenSource().token,
    );

    let result = "";
    for await (const fragment of response.text) {
      result += fragment;
    }

    return result;
  }

  private async getAvailableModel() {
    // å°è¯•è·å– Copilot æ¨¡å‹
    const models = await vscode.lm.selectChatModels({
      vendor: "copilot",
      family: "gpt-4o", // ä¼˜å…ˆä½¿ç”¨ GPT-4o
    });

    // å¦‚æœæ²¡æœ‰ GPT-4oï¼Œå°è¯•å…¶ä»–æ¨¡å‹
    let model = models[0];
    if (!model) {
      const fallbackModels = await vscode.lm.selectChatModels({
        vendor: "copilot",
      });
      model = fallbackModels[0];
    }

    if (!model) {
      throw new Error(
        `æ²¡æœ‰å¯ç”¨çš„ ${PROVIDER_NAMES.COPILOT} æ¨¡å‹ã€‚è¯·ç¡®ä¿å·²å®‰è£…å¹¶ç™»å½• GitHub Copilotã€‚`,
      );
    }

    return model;
  }
}
